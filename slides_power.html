<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>RETROSPECTIVE &amp; PROSPECTIVE POWER ANALYSES FOR ENVIRONMENTAL ECONOMICS</title>
    <meta charset="utf-8" />
    <meta name="author" content="LÃ©o Zabrocki https://lzabrocki.github.io/ leo.zabrocki@psemail.eu" />
    <meta name="date" content="2022-06-06" />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: top, left, title-slide

.title[
# RETROSPECTIVE &amp; PROSPECTIVE POWER ANALYSES FOR ENVIRONMENTAL ECONOMICS
]
.subtitle[
## <span style="color:white">Alliance Graduate Summer School 2022</span>
]
.author[
### LÃ©o Zabrocki<br><a href="https://lzabrocki.github.io/" class="uri">https://lzabrocki.github.io/</a><br><a href="mailto:leo.zabrocki@psemail.eu" class="email">leo.zabrocki@psemail.eu</a>
]
.institute[
### <span style="color:white">PSE - EIEE</span>
]
.date[
### <span style="color:white">2022-06-06</span>
]

---


&lt;style&gt;
.center2 {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}
&lt;/style&gt;





---
class: inverse, center, middle
background-color: #272822;

# Is a Replicability Crisis on the Horizon for Environmental and Resource Economics?

---
# Paul J. Ferraro and Pallavi Shukla (2020)

&lt;img src="slides_power_files/figure-html/unnamed-chunk-1-1.png" width="150%" style="display: block; margin: auto;" /&gt;


---
# The Null Hypothesis Significance Testing Framework (NHST)


.pull-left[
&lt;img src="https://rss.onlinelibrary.wiley.com/cms/asset/9ec91b71-2007-4609-b6a0-f88abac4e39b/sign619-gra-0001-m.jpg" width="68%" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;br&gt;
* *Every experiment may be said to exist only to give the facts a chance of disproving the null hypothesis.*

* Actually not a very interesting hypothesis

* Very bad consequences for research

* People are obsessed with p-values

* Dichotomize evidence according to them

* Leads to publication bias
]

---
# NHST + Publication Bias = ðŸ˜±


.pull-left[

**Paul J. Ferraro and Pallavi Shukla (2020):**

* Median Power is 33%

* 56% of reported estimates are 2 times too large

* 35% are exaggerated by a factor of 4 or more

]
.pull-right[
&lt;img src="https://media.giphy.com/media/RX3vhj311HKLe/giphy.gif" width="100%" style="display: block; margin: auto;" /&gt;
]
&lt;br&gt;
&lt;br&gt;
&lt;p style="text-align: center;color:#ffbf69;"&gt;&lt;b&gt;&lt;big&gt;WHEN STUDIES HAVE A LOW STATISTICAL POWER, THEY MUST FIND LARGE EFFECT SIZES TO PASS THE SIGNIFICANCE FILTER&lt;/big&gt;&lt;/b&gt;&lt;/p&gt;

---

![](figures/winner_curse.png)


---
background-image: url(https://camo.githubusercontent.com/b1643d972b1c1dc918b6b17e389349208d68ab37/68747470733a2f2f692e70696e696d672e636f6d2f6f726967696e616c732f31362f31322f38372f31363132383765323135643038333966373566333264363936323563643130322e676966)
background-position: center
background-size: contain

---
# Today's Battle Plan

.pull-left[
&lt;br&gt;
&lt;br&gt;
1. A **toy example** for understanding the issues raised by low power

2. **Prospective power analysis** to design better powered studies

3. **Retrospective power analysis** to evaluate if a study suffers from these issues
]
.pull-right[
&lt;img src="https://m.media-amazon.com/images/M/MV5BNzg2NTA2MTU4OF5BMl5BanBnXkFtZTcwMDkxNTcyOQ@@._V1_.jpg" width="100%" style="display: block; margin: auto;" /&gt;
]



---
# Software Requirements

.pull-left[
**Installing R &amp; RStudio:**

- R : https://cran.r-project.org

- RStudio: https://rstudio.com/

**Packages:**

- `here`: for file paths management
- `tidyverse`: for data wrangling &amp; visualization
- `broom`: for cleaning regression outputs
- `AER`: for running IV models
- `retrodesign`: for retro power analysis
]
.pull-right[
&lt;br&gt;
&lt;br&gt;
&lt;img src="https://media.giphy.com/media/aNqEFrYVnsS52/giphy.gif" width="100%" /&gt;
]


---
# Coding

&lt;br&gt;
&lt;br&gt;
* GitHub link:

* **Download the folder!**

* Folder **scripts_tutorial** contains all the R scripts

* `slides_power.html` are the slides of the workshop

* `tutorial_power.html` explains with texts all the stuff covered today


---
class: inverse, center, middle
background-color: #272822;

# Why P-Values Are Misleading in Low Power Studies


---
# An Agricultural Experiment

.pull-left[
* Draw inspiration from a recent study by [Jacob R. Pecenka et al. (2021)](https://www.pnas.org/doi/10.1073/pnas.2108429118#abstract)

* Compared to conventional practices (CM), an integrated pest management system (IPM) can **reduce insecticide use by 95%** and even **increase the yields of food crops** since wild bees are conserved. 

* `\(N\)` fields are randomly allocated either to IPM or CM. 

* For each field, the yield of watermelons in `\(kg/m^2\)` is measured. 
]
.pull-right[
&lt;br&gt;
&lt;br&gt;
&lt;img src="https://rss.onlinelibrary.wiley.com/cms/asset/9d3c14b5-058c-4ab4-b0b6-dc35e28aa14a/sign1144-gra-0004-m.jpg" width="100%" /&gt;
]

---
# Simulating the Data

&lt;br&gt;
If we denote `\(i\)` the index of a field, the DGP can be summarized by:

`\begin{equation*}
Y_{i}(W_i=1) = Y_{i}(W_i=0) + \tau_{i}
\end{equation*}`

* `\(Y_{i}(0)\sim N(14, 5)\)` is the potential watermelon yields when the field is not treated: `\((W_i=0)\)`. 

* For each field `\(i\)`, the potential watermelon yields when the field is treated `\((W_i=1)\)` is created by drawing a treatment unit-causal effect from `\(\tau_{i} \sim N(1, 3)\)`. 

* This DGP is the classic additive treatment effect model under Normality.

---
# In R

Open `script_introductory_example.R`

If they are installed, load the following packages:


```r
library(here) # for file paths management
library(tidyverse) # for data manipulation and visualization
```

---
# Function to Simulate the Science Table

`function_science_table()` creates the data:


```r
# function to create the science table
function_science_table &lt;- function(sample_size) {
  # define field index and y(0)
  tibble(field = c(1:sample_size),
         y_0 = rnorm(sample_size, 14, 5)) %&gt;%
    # define y(1) by adding treatment effect size
    mutate(y_1 = y_0 + rnorm(sample_size, 1, 3)) %&gt;%
    # round values of potential outcomes
    mutate_at(vars(y_0, y_1), ~ round(., 1))
}
```

---
# Running the Function

We run the function for a sample size of 50 fields:


```r
# set seed
set.seed(42)

# create a science table for 100 fields
data_science &lt;- function_science_table(50)

# print table
data_science %&gt;% slice(1:5)
```

```
## # A tibble: 5 x 3
##   field   y_0   y_1
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1  20.9  22.8
## 2     2  11.2   9.8
## 3     3  15.8  21.5
## 4     4  17.2  20.1
## 5     5  16    17.3
```

---
&lt;img src="slides_power_files/figure-html/unnamed-chunk-10-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---
# Computing the True ATE

As we observe both potential outcomes for each field, we can compute the true value of the finite sample average treatment effect `\(\tau_{fs}\)` as:

`\begin{equation*}
\tau_{fs} = \frac{1}{N} \sum_{i=1}^{N}\left(Y_i(1) - Y_i(0) \right)
\end{equation*}`


```r
# compute value of the ate
ate &lt;- data_science %&gt;%
  mutate(tau = y_1 - y_0) %&gt;%
  summarise(mean(tau) %&gt;% round(., 1)) %&gt;%
  pull()

# display the value
ate
```

```
## [1] 1.3
```

---
# Running One Iteration of the Experiment


```r
# load one iteration of the experiment
data_science_one_iteration &lt;- readRDS(here::here("data", 
                                                 "data_science_one_iteration.rds"))
```

Once the treatment has been allocated, we express the observed watermelon yields `\(Y^{obs}\)` such that:

`\begin{equation*}
Y^{obs}_{i} = W_{i}\times Y_{i}(1) + (1-W_{i})\times Y_{i}(0)
\end{equation*}`


```r
# express observed yields
data_science_one_iteration &lt;- data_science_one_iteration %&gt;%
  mutate(y_obs = w * y_1 + (1 - w) * y_0)
```

---
# Estimating the ATE &amp; Computing 95% CI
To estimate the average causal effect, we compute the difference in the average yields between treated and control units `\(\hat{\tau}\)`:

`\begin{equation*}
\hat{\tau} = \frac{1}{N_{t}} \sum_{i|W_{i}=1} Y_i^{obs} - \frac{1}{N_{c}} \sum_{i|W_{i}=0} Y_i^{obs}
\end{equation*}`

where `\(N_{t}\)` and `\(N_{c}\)` are respectively the sample size of treated and control groups. To compute the variance of the estimator `\(\hat{\tau}\)`, we use Neyman's estimator:


`\begin{equation*}
\hat{\mathbb{V}}(\hat{\tau}) = \frac{\frac{1}{N_{c}-1} \sum_{i|W_{i}=0} \left(Y_i^{obs} - \bar{Y_{c}} \right)^2}{N_{c}} + \frac{\frac{1}{N_{t}-1} \sum_{i|W_{i}=1} \left(Y_i^{obs} - \bar{Y_{t}} \right)^2}{N_{t}}
\end{equation*}`

And compute a 95% confidence intervals with a normal approximation:

`\begin{equation*}
\text{CI}_{0.95}(\tau_{fs}) = (\hat{\tau}-1.96\sqrt{\hat{\mathbb{V}}}, \hat{\tau}+1.96\sqrt{\hat{\mathbb{V}}})
\end{equation*}`


---
# In R
We can compute the point estimate for the treatment effect and the 95% confidence interval with the following code:


```r
# compute estimate for ate
estimate_ate &lt;- data_science_one_iteration %&gt;%
  group_by(w) %&gt;%
  summarise(group_mean = mean(y_obs)) %&gt;%
  summarise(group_mean[2] - group_mean[1]) %&gt;%
  pull()
```

---
# In R


```r
# compute associated standard error
standard_error &lt;- data_science_one_iteration %&gt;%
  group_by(w) %&gt;%
  mutate(
    n_obs = n(),
    average_y_obs = mean(y_obs),
    squared_difference = (y_obs - average_y_obs) ^
      2,
    variance_group = sum(squared_difference) / (n_obs - 1)
  ) %&gt;%
  summarise(variance_component = mean(variance_group / n_obs)) %&gt;%
  summarise(variance = sum(variance_component),
            standard_error = sqrt(variance)) %&gt;%
  pull(standard_error)
```

---
# In R


```r
# return ate estimate and 95% confidence interval
results_one_iteration &lt;- tibble(
  estimate_ate = estimate_ate,
  lower_95_ci = estimate_ate - 1.96 * standard_error,
  upper_95_ci = estimate_ate + 1.96 * standard_error
) %&gt;%
  mutate_all( ~ round(., 1))

# print results
results_one_iteration
```

```
## # A tibble: 1 x 3
##   estimate_ate lower_95_ci upper_95_ci
##          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
## 1          3.4         0.3         6.5
```

---
# Replicating Many Times the Experiment

To save time, I have already stored the results:


```r
# load 1000 iterations of the experiment
data_results_1000_experiments &lt;- readRDS(here::here("data",
                                                    "data_results_1000_experiments.rds"))

# add iteration index
data_results_1000_experiments &lt;- data_results_1000_experiments %&gt;%
  mutate(iteration = 1:1000) %&gt;%
  mutate(significant = ifelse(significant == 1, "True", "False"))
```

---

&lt;img src="slides_power_files/figure-html/unnamed-chunk-18-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;img src="slides_power_files/figure-html/unnamed-chunk-19-1.png" width="120%" style="display: block; margin: auto;" /&gt;


---
# Defining &amp; Computing Statistical Power

First, the statistical power for rejecting the null hypothesis `\(H_{0} : \tau_{fs} = 0\)` is given by:

`\begin{equation*}
\Phi(âˆ’1.96 âˆ’ \frac{\tau_{fs}}{s}) + 1 âˆ’ \Phi(1.96 âˆ’ \frac{\tau_{fs}}{s})
\end{equation*}`

with `\(s\)` the standard error of our estimate `\(\hat{\tau}\)` and `\(\Phi\)` the cumulative function of the standard normal distribution.


```r
# retrieve standard error of the first experiment iteration
s &lt;- (results_one_iteration$upper_95_ci - results_one_iteration$estimate_ate) / 1.96

# compute power using its definition
(pnorm(-1.96 - 1.3 / s) + 1 - pnorm(1.96 - 1.3 / s)) * 100
```

```
## [1] 13.02486
```

---
# Computing Power with Simulation Results

```r
# compute power using simulation results
power_simulation &lt;- data_results_1000_experiments %&gt;%
  summarise(statistical_power = round(sum(significant == "True") / n() * 100, 0))

# print result
power_simulation
```

```
## # A tibble: 1 x 1
##   statistical_power
##               &lt;dbl&gt;
## 1                10
```

---
# Defining &amp; Computing Type M Error

The average Type M error is defined as:
`\begin{equation*}
\mathbb{E}\left(\frac{|\hat{\tau}|}{|\tau_{fs}|} | \tau_{fs}, s, |\hat{\tau}|/s&gt;1.96 \right)
\end{equation*}`


```r
# compute average type m error
type_m &lt;- data_results_1000_experiments %&gt;%
  filter(significant == "True") %&gt;%
  summarise(exageration_factor = mean(abs(estimate_ate) / ate) %&gt;% round(., 1))

# display result
type_m
```

```
## # A tibble: 1 x 1
##   exageration_factor
##                &lt;dbl&gt;
## 1                  3
```

---
# Defining &amp; Computing Type S Error

The probability to make a type S error is:

`\begin{equation*}
\frac{\Phi(-1.96-\frac{\tau_{fs}}{s})}{1-\Phi(1.96-\frac{\tau_{fs}}{s}) + \Phi(-1.96 - \frac{\tau_{fs}}{s})}
\end{equation*}`


```r
# compute probability to make a type s error
type_s &lt;- data_results_1000_experiments %&gt;%
  filter(significant == "True") %&gt;%
  summarise(probability_type_s_error = round(sum(estimate_ate &lt; 0) / n() * 100, 1))

# display result
type_s
```

```
## # A tibble: 1 x 1
##   probability_type_s_error
##                      &lt;dbl&gt;
## 1                      1.9
```

---
# How Type M and S Errors Evolve with Power

To save time, I have already stored the results:


```r
# load simulations of the experiment for increasing sample sizes
data_power_sample_size_sim &lt;- readRDS(here::here("data", "data_power_sample_size_sim.rds"))
```

For each sample size, we compute the power, type M and S errors:


```r
# compute power, type m and s errors
summary_power_sample_size_sim &lt;- data_power_sample_size_sim %&gt;%
  group_by(sample_size) %&gt;%
  summarise(
    power = mean(significant == 1, na.rm = TRUE) * 100,
    type_m = mean(ifelse(
      significant == 1, abs(estimate_ate / ate), NA
    ), na.rm = TRUE),
    type_s = sum(ifelse(
      significant == 1, sign(estimate_ate) != sign(ate), NA
    ), na.rm = TRUE) / n() * 100
  )
```

---
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;img src="slides_power_files/figure-html/unnamed-chunk-26-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
class: inverse, center, middle
background-color: #272822;

# Prospective Power Analysis

---
# Standard Approach

Statistical power is defined as:

`\begin{equation*}
\beta = \Phi\left(\frac{|\mu_{t}-\mu_{c}|\sqrt{N}}{2\sigma} - \Phi^{-1}(1-\frac{\alpha}{2})\right)
\end{equation*}`

* `\(Phi\)` is the cumulative distribution function of the normal distribution.

* `\(\mu_t\)` and `\(mu_c\)` are the average outcomes of the treatment and control groups.

* `\(\sigma\)` is the standard deviation of the outcome. Here, we assume that it is the same for both groups.

* `\(\alpha\)` is the significance level.

* `\(N\)` is the number of units.

---
# Simulation-Based Approach: Health Effects of Forest Fires

&lt;br&gt;
&lt;br&gt;
&lt;img src="figures/flowchart_simulations.png" width="100%" /&gt;

---
# In R

Open `script_prospective_power_analysis.R`

If they are installed, load the following packages:


```r
library(here) # for file paths management
library(tidyverse) # for data manipulation and visualization
library(AER) # for running iv model
library(broom) # for cleaning statistical model output
```

The coding workflow of this section was designed by [Vincent Bagilet](https://vincentbagilet.github.io/causal_inflation/) and relies heavily on the `purrr` package.

---
# Defining the DGP
.pull-left[

**Model for emergency admissions:**

`\begin{equation*}
Emergency_{t} = \alpha + \beta Pollution_{t} + \delta u_{t} + e^{(E)}_{t}
\end{equation*}`

**Model for air pollution:**

`\begin{equation*}
Pollution_{t} = \gamma + \lambda Fire_{t} + \eta u_{t} + e^{(P)}_{t}
\end{equation*}`

* `\(t\)` the day index
* `\(Fire \sim \text{Gamma}(k, \theta)\)`.
* `\(u \sim \mathcal{N}(0, \sigma_{u}^{2})\)` the unobserved variable.
* `\(e^{(S)} \sim \mathcal{N}(0, \sigma_{e_S}^{2})\)`.
* `\(e^{(T)} \sim \mathcal{N}(0, \sigma_{e_T}^{2})\)`.
]
.pull-right[
&lt;img src="figures/sumatra.jpg" width="65%" style="display: block; margin: auto;" /&gt;
]


---
# Function to Generate the Data

```r
# generate IV data
generate_data_IV &lt;- function(N,
                             param_fire,
                             sigma_u,
                             sigma_ee,
                             sigma_ep,
                             alpha,
                             gamma,
                             treatment_effect,
                             iv_strength,
                             ovb_intensity) {
  # first create day index
  data &lt;- tibble(id = 1:N) %&gt;%
    mutate(
      # create fire radiative power from a gamma distribution
      fire = rgamma(N, shape = param_fire[1], scale = param_fire[2])*50000,
      # create the unobserved variable
      u = rnorm(nrow(.), 0, sigma_u),
      # random noise for the emergency model
      e_e = rnorm(nrow(.), 0, sigma_ee),
      # random noise for the air pollution model
      e_p = rnorm(nrow(.), 0, sigma_ep),
      # create air pollution variable
      pollution = gamma + iv_strength * fire + ovb_intensity * u + e_p,
      # create emergency admission variable
      emergency = alpha + treatment_effect * pollution + ovb_intensity * u + e_e
    )
  
  return(data)
}
```

---
# Setting the Values


```r
baseline_param_IV &lt;- tibble(
  N = 3650,
  param_fire = list(c(1, 2)),
  sigma_u = 1,
  sigma_ee = 100,
  sigma_ep = 5,
  alpha = 400,
  gamma = 30,
  treatment_effect = 1,
  iv_strength = 10/100000, 
  ovb_intensity = 0.4
)
```


---
# Simulating One Dataset


```r
# set seed
set.seed(42)

# simulate one dataset
data_fire &lt;- baseline_param_IV %&gt;%
  pmap_dfr(generate_data_IV)
```

---
# Relationship between Air Pollution &amp; Forest Fire


```r
# check effect of forest fires on air pollution
data_fire %&gt;%
  mutate_at(vars(fire, pollution), ~ scale(.)) %&gt;%
  lm(pollution ~ fire + u, data = .) %&gt;%
  broom::tidy(., conf.int = TRUE) %&gt;%
  filter(term == "fire")
```

```
## # A tibble: 1 x 7
##   term  estimate std.error statistic p.value conf.low conf.high
##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 fire     0.894   0.00740      121.       0    0.880     0.909
```

A one standard deviation increase in the fire radiative power increases the concentration of `\(\text{PM}_{10}\)` by about **0.46** standard deviation.

---
# Relationship between Air Pollution &amp; Emergency


```r
# check effect of instrumented air pollution on emergency admission
data_fire %&gt;%
  mutate_at(vars(fire, pollution), ~ scale(.)) %&gt;%
  AER::ivreg(data = .,
                       formula = log(emergency) ~ pollution | fire) %&gt;%
  broom::tidy(., conf.int = TRUE) %&gt;%
  filter(term == "pollution")
```

```
## # A tibble: 1 x 7
##   term      estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 pollution   0.0325   0.00465      6.99 3.24e-12   0.0234    0.0416
```

A one standard deviation in instrumented air pollution increases emergency admission by **6%**.

---
# Function for Estimating IV Model

We create below the function `estimate_IV()` to estimate the IV model in our simulations:


```r
# estimate IV model
estimate_IV &lt;- function(data) {
  reg_IV &lt;- AER::ivreg(data = data,
                       formula = emergency ~ pollution | fire)
  
  fstat_IV &lt;- summary(reg_IV,
                      diagnostics = TRUE)$diagnostics["Weak instruments", "statistic"]
  
  reg_IV &lt;- reg_IV %&gt;%
    broom::tidy(., conf.int = TRUE) %&gt;%
    mutate(fstat = fstat_IV) %&gt;%
    filter(term == "pollution") %&gt;%
    rename(p_value = p.value, se = std.error, conf_low = conf.low, conf_high = conf.high) %&gt;%
    select(estimate, p_value, se, conf_low, conf_high, fstat) %&gt;%
    
    return(reg)
}
```

---
# Function to Run the Simulations

We combine the functions `generate_data_IV` and `estimate_IV` into a single function `compute_sim_IV` to run our simulations:


```r
# define compute_sim_IV
compute_sim_IV &lt;- function(N,
                           param_fire,
                           sigma_u,
                           sigma_ee,
                           sigma_ep,
                           alpha,
                           gamma,
                           treatment_effect,
                           iv_strength,
                           ovb_intensity) {
  generate_data_IV(
    N = N,
    param_fire = param_fire,
    sigma_u = sigma_u,
    sigma_ee = sigma_ee,
    sigma_ep = sigma_ep,
    alpha = alpha,
    gamma = gamma,
    treatment_effect = treatment_effect,
    iv_strength = iv_strength,
    ovb_intensity = ovb_intensity
  ) %&gt;%
    estimate_IV() %&gt;%
    mutate(
      iv_strength = iv_strength,
      ovb_intensity = ovb_intensity,
      true_effect = treatment_effect
    )
} 
```

---
# Varying Parameters

```r
# set vector of iv strengths
vect_iv_strength &lt;- c(1, 2, 3, 4, 5, 10) / 100000

# number of iterations
n_iter &lt;- 500

# create parameters of the simulations
param_IV &lt;- baseline_param_IV %&gt;%
  select(-iv_strength) %&gt;%
  crossing(vect_iv_strength) %&gt;%
  rename(iv_strength = vect_iv_strength) %&gt;%
  crossing(rep_id = 1:n_iter) %&gt;%
  select(-rep_id)
```

---
# Results

To save time, we have already the simulations and we load the results:


```r
# load simulations data
sim_IV &lt;- readRDS(here::here("data", "data_sim_power_fire.rds"))
```

We define the `summarise_simulations()` to compute power, type M error, the first-stage F-statistics but also the mean width of 95% confidence intervals:


```r
# summary relevant metrics
summarise_simulations &lt;- function(data) {
  data %&gt;%
    mutate(significant = (p_value &lt;= 0.05)) %&gt;% 
    group_by(ovb_intensity, iv_strength) %&gt;%
    summarise(
      power = mean(significant, na.rm = TRUE)*100, 
      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),
      mean_fstat = mean(fstat, na.rm = TRUE),
      mean_width_ci = mean(abs(conf_high - conf_low)),
      .groups	= "drop"
    ) %&gt;% 
    ungroup()
} 
```

---
# Computing Relevant Metrics

We run the function:


```r
# run summarise_simulations
summary_sim_IV &lt;- summarise_simulations(sim_IV)
```

And display below the results:


```
## # A tibble: 6 x 5
##   `Strength of the IV` `Power (%)` `Type M Errror` `F-Statistic` `Width of 95% C~
##                  &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;
## 1              0.00001         9.6             4.1           145              6.6
## 2              0.00002        22.4             2.1           581              3.3
## 3              0.00003        42.6             1.5          1304              2.2
## 4              0.00004        66.8             1.2          2312              1.6
## 5              0.00005        85.6             1.1          3622              1.3
## 6              0.0001        100               1           14547              0.6
```


---
class: inverse, center, middle
background-color: #272822;

# Retrospective Power Analysis

---
# Intuition

* Imagine that we have analyzed observational data with a particular causal inference method. 

* The true causal effect is `\(\beta\)` and we found an estimate `\(\hat{\beta}\)` with a standard error `\(s\)`. 

* The estimate is statistically significant at the 5% level.

* Computing the statistical power but also type M and S errors, only requires to draw many estimates from a `\(N(\beta, s)\)`

* We proceed as if we were able to replicate the study many times but under the assumption that the value of the causal estimand is different from the observed estimate.

* In R, the [retrodesign](https://cran.r-project.org/web/packages/retrodesign/vignettes/Intro_To_retrodesign.html) package implement the closed-form expression derived by [Lu et al. (2019)](https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bmsp.12132) to avoid relying on simulations.



---
# In R

Open `script_retrospective_power_analysis.R`

If they are installed, load the following packages:


```r
library(here) # for file paths management
library(tidyverse) # for data manipulation and visualization
library(retrodesign) # for retrospective power analysis
```

---
# Heat and Learning

.pull-left[
[R. Jisung Park et al. (2020)](https://pubs.aeaweb.org/doi/pdfplus/10.1257/pol.20180612):

*"On average in the United States, experiencing **a 1Â°F hotter school year lowers academic achievement by 0.002 standard deviations** [...] the average gain in PSAT score performance between tenth and eleventh grade is about 0.3 standard deviations, implying that **a 1Â°F hotter school year reduces learning by close to 1 percent of the expected gains over that year**"*


20 millions observations

**95% CI: (-0.0030, -0.0015)**
]
.pull-right[
&lt;img src="https://i.guim.co.uk/img/media/601b5cb816dde152712d472e740ab00f46960f69/0_308_2732_1639/master/2732.jpg?width=1200&amp;height=1200&amp;quality=85&amp;auto=format&amp;fit=crop&amp;s=51953c22eec2ecad7f81f3a4688c75f7" width="80%" style="display: block; margin: auto;" /&gt;
]

---
# Reducing the Estimate by 25%

First, we could check what would happen if the true effect size was actually 25% smaller than the observed estimate. 

Using the **retro_design()** function, we only need to give a list of potential true values *A* and the value of the standard error *s* of the point estimate.


```r
# compute power for a reduction of 25% of the point estimate
retro_design(A = -0.0023 * 0.75,
             s = 0.00042,
             alpha = 0.05) %&gt;% as_tibble() %&gt;% mutate_all( ~ round(., 2))
```

```
## # A tibble: 1 x 3
##   power typeS typeM
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  0.98     0  1.01
```

---
# Taking the lower bound of the 95% CI

As a second check, we could take the lower bound of the 95% confidence interval as the true value of the point estimate: it represents a decrease by 0.5% of the expected gains in learning over a year. 

We could argue that the authors would have still published their paper with such an effect size.


```r
# compute power for lower bound of 95% ci
retro_design(A = -0.0015,
             s = 0.00042,
             alpha = 0.05) %&gt;% as_tibble() %&gt;% mutate_all( ~ round(., 2))
```

```
## # A tibble: 1 x 3
##   power typeS typeM
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  0.95     0  1.03
```

---
# Skeptical Readers

Finally, other researchers could be really skeptic about the results of [R. Jisung Park et al. (2020)](https://pubs.aeaweb.org/doi/pdfplus/10.1257/pol.20180612) and claim that the true effect size should be closer to a 0.1% decrease of the expected gains in learning:


```r
# compute power for an effect size of -0.1%
retro_design(A = -0.0003,
             s = 0.00042,
             alpha = 0.05) %&gt;% as_tibble() %&gt;% mutate_all( ~ round(., 2))
```

```
## # A tibble: 1 x 3
##   power typeS typeM
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  0.11  0.03  3.41
```

---
# Plotting Power, Type M and S Errors

&lt;img src="slides_power_files/figure-html/unnamed-chunk-46-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
class: inverse, center, middle
background-color: #272822;

# General Recommandations

---
# Bias in Observational Studies

.pull-left[
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
**King and Zeng (2007) decompose bias into:**

1. Omitted Variable Bias

2. Post-Treatment Bias

3. Interpolation Bias

4. Extrapolation Bias
]
.pull-right[
&lt;img src="figures/jason_momoa.jpg" width="60%" style="display: block; margin: auto;" /&gt;
]

---
# Three Recommandations

.pull-left[
&lt;br&gt;
&lt;big&gt;1. Abandon statistical significance&lt;/big&gt; [(McShane et al., 2019)](https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1527253)

&lt;big&gt;2. Replicate studies, even if they are under-powered&lt;/big&gt; [(Miguel A.HernÃ¡n, 2021)](https://www.sciencedirect.com/science/article/pii/S0895435621002730)

&lt;big&gt;3. Interpret the width of 95% confidence intervals&lt;/big&gt; [(Valentin Amrhein et al., 2019)](https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1543137)
]
.pull-right[
&lt;img src="figures/feynman_old.jpg" width="65%" style="display: block; margin: auto;" /&gt;
]
&lt;p style="text-align: center;color:#348aa7"&gt;&lt;b&gt;&lt;big&gt;&lt;i&gt;â€œI think that when we know that we
actually do live in uncertainty, then
we ought to admit it."&lt;/i&gt;&lt;/big&gt;&lt;/b&gt;&lt;/p&gt;



---
class: center, middle

![](figures/anakin_padme.jpg)
---
class: center

# THANK YOU FOR ATTENDING THIS WORKSHOP!


&lt;img src="https://media.giphy.com/media/ZfK4cXKJTTay1Ava29/giphy.gif" width="45%" style="display: block; margin: auto;" /&gt;

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "solarized-light",
"highlightLines": true,
"countIncrementalSlides": false,
"exclude": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
